task_id: "task_id"
run_id: "run_id"
num_epochs: 1
dataset_dir: "datasets/docker"
batch_size: 64
learning_rate: 1e-2
reuse_epoch: True
dataloader_workers: 4
loss:
  type: triplet
  margin: 0.1
negatives:
  type: batch
text_vectorizer:
  type: dictionary
  dictionary_path: datasets/docker/token_dictionary_150k_0k_0k
  word_unigrams_limit: 8
  word_bigrams_limit: 0
  char_trigrams_limit: 0
  num_oov_tokens: 50000
model:
  type: SimpleTextEncoder
  text_embedding:
    num_embeddings: 200001
    embedding_dim: 256
  fc_dim: 128
  output_dim: 128
validate_recall: True
